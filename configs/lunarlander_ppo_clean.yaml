# Stage A: Clean baseline PPO configuration (no DR/wind)
env_id: LunarLander-v3
algo: ppo
seed: 0
n_envs: 16
vecnormalize: false
timesteps: 3000000
device: cuda
use_curriculum: false
use_randomization: false
wind:
  p: 0.0

# Policy configuration
policy: MlpPolicy
policy_kwargs:
  net_arch: [64, 64]
  activation_fn: "tanh"

# PPO hyperparameters
learning_rate: 2.5e-4
lr_schedule: linear
n_steps: 2048
batch_size: 64
gamma: 0.99
gae_lambda: 0.95
clip_range: 0.2
ent_coef: 0.01
vf_coef: 0.5
max_grad_norm: 0.5

# Evaluation settings
eval_freq: 10000
n_eval_episodes: 100
eval_deterministic: true

# Logging
log_interval: 1
save_freq: 50000
verbose: 1